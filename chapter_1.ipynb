{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Perceptron](http://neuralnetworksanddeeplearning.com/chap1.html#perceptrons)\n",
    "A perceptron takes several binary inputs, x1,x2,…, and produces a single binary output:\n",
    "![image.png](http://neuralnetworksanddeeplearning.com/images/tikz0.png)\n",
    "\n",
    "\n",
    "# [Sigmoid neurons](http://neuralnetworksanddeeplearning.com/chap1.html#sigmoid_neurons)\n",
    "Sigmoid neurons are similar to perceptrons, but modified so that small changes in their weights and bias cause only a small change in their output. That's the crucial fact which will allow a network of sigmoid neurons to learn.\n",
    "![image.png](http://neuralnetworksanddeeplearning.com/images/tikz9.png)\n",
    "\n",
    "\n",
    "\\begin{eqnarray} \n",
    "  \\frac{1}{1+\\exp(-\\sum_j w_j x_j-b)}.\n",
    "\\end{eqnarray}\n",
    "\n",
    "To understand the similarity to the perceptron model, suppose z≡w⋅x+b is a large positive number. Then e−z≈0 and so σ(z)≈1. In other words, when z=w⋅x+b is large and positive, the output from the sigmoid neuron is approximately 1, just as it would have been for a perceptron. Suppose on the other hand that z=w⋅x+b is very negative. Then e−z→∞, and σ(z)≈0. So when z=w⋅x+b is very negative, the behaviour of a sigmoid neuron also closely approximates a perceptron. It's only when w⋅x+b is of modest size that there's much deviation from the perceptron model.\n",
    "\n",
    "If it's the shape of σ which really matters, and not its exact form, then why use the particular form used for σ in Equation (3)? In fact, later in the book we will occasionally consider neurons where the output is f(w⋅x+b) for some other activation function f(⋅).\n",
    "## [Exercises](http://neuralnetworksanddeeplearning.com/chap1.html#exercises_191892)\n",
    "* **Sigmoid neurons simulating perceptrons, part I**\n",
    "Suppose we take all the weights and biases in a network of perceptrons, and multiply them by a positive constant, c>0. Show that the behaviour of the network doesn't change.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\mbox{output} = \\left\\{ \n",
    "    \\begin{array}{ll} \n",
    "      0 & \\mbox{if } c\\cdot w\\cdot x + c\\cdot b \\leq 0 \\\\\n",
    "      1 & \\mbox{if } c\\cdot w\\cdot x + c\\cdot b > 0\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "\\end{eqnarray}\n",
    "\\begin{eqnarray}\n",
    "  \\mbox{output} = \\left\\{ \n",
    "    \\begin{array}{ll} \n",
    "      0 & \\mbox{if } c ( w\\cdot x + b) \\leq 0 \\\\\n",
    "      1 & \\mbox{if } c ( w\\cdot x + b) > 0\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "\\end{eqnarray}\n",
    "\\begin{eqnarray}\n",
    "  \\mbox{output} = \\left\\{ \n",
    "    \\begin{array}{ll} \n",
    "      0 & \\mbox{if } w\\cdot x + b \\leq 0 \\\\\n",
    "      1 & \\mbox{if } w\\cdot x + b > 0\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "\\end{eqnarray}\n",
    "\n",
    "* **Sigmoid neurons simulating perceptrons, part II**\n",
    "Suppose we have the same setup as the last problem - a network of perceptrons. Suppose also that the overall input to the network of perceptrons has been chosen. We won't need the actual input value, we just need the input to have been fixed. Suppose the weights and biases are such that w⋅x+b≠0 for the input x to any particular perceptron in the network. Now replace all the perceptrons in the network by sigmoid neurons, and multiply the weights and biases by a positive constant c>0. Show that in the limit as c→∞ the behaviour of this network of sigmoid neurons is exactly the same as the network of perceptrons. How can this fail when w⋅x+b=0 for one of the perceptrons?\n",
    "\n",
    "\\begin{eqnarray} \n",
    "  \\frac{1}{1+\\exp(-\\sum_j c_j (w_j x_j-b))}.\n",
    "\\end{eqnarray}\n",
    "with c→∞, then if:\n",
    "\\begin{array}{ll} \n",
    "  \\mbox{if } w\\cdot x + b \\leq 0 \\\\\n",
    "\\end{array}\n",
    "then\n",
    "\\begin{eqnarray} \n",
    "  \\frac{1}{1+\\exp(-\\sum_j c_j (w_j x_j-b))} \\approx 0.\n",
    "\\end{eqnarray}\n",
    "if:\n",
    "\\begin{array}{ll} \n",
    "  \\mbox{if } w\\cdot x + b > 0 \\\\\n",
    "\\end{array}\n",
    "then\n",
    "\\begin{eqnarray} \n",
    "  \\frac{1}{1+\\exp(-\\sum_j c_j (w_j x_j-b))} \\approx 1.\n",
    "\\end{eqnarray}\n",
    "\n",
    "# [The architecture of neural networks](http://neuralnetworksanddeeplearning.com/chap1.html#the_architecture_of_neural_networks)\n",
    "![image.png](http://neuralnetworksanddeeplearning.com/images/tikz11.png)\n",
    "* The leftmost layer in this network is called the **input layer**, and the neurons within the layer are called **input neurons**. \n",
    "* The rightmost or **output layer** contains the **output neurons**, or, as in this case, a single output neuron. \n",
    "* The middle layer is called a **hidden layer**, since the neurons in this layer are neither inputs nor outputs.\n",
    "\n",
    "such multiple layer networks are sometimes called multilayer perceptrons or **MLPs**, despite being made up of sigmoid neurons, not perceptrons.\n",
    "\n",
    "We've been discussing neural networks where the output from one layer is used as input to the next layer. Such networks are called **feedforward neural networks**. This means there are no loops in the network - information is always fed forward, never fed back.\n",
    "\n",
    "However, there are other models of artificial neural networks in which feedback loops are possible. These models are called **recurrent neural networks**. The idea in these models is to have neurons which fire for some limited duration of time, before becoming quiescent.\n",
    "\n",
    "They're much closer in spirit to how our brains work than feedforward networks. And it's possible that recurrent networks can solve important problems which can only be solved with great difficulty by feedforward networks.\n",
    "\n",
    "# [A simple network to classify handwritten digits](http://neuralnetworksanddeeplearning.com/chap1.html#a_simple_network_to_classify_handwritten_digits)\n",
    "\n",
    "## [Exercise](http://neuralnetworksanddeeplearning.com/chap1.html#exercise_513527)\n",
    "* There is a way of determining the bitwise representation of a digit by adding an extra layer to the three-layer network above. The extra layer converts the output from the previous layer into a binary representation, as illustrated in the figure below. Find a set of weights and biases for the new output layer. Assume that the first 3 layers of neurons are such that the correct output in the third layer (i.e., the old output layer) has activation at least 0.99, and incorrect outputs have activation less than 0.01.\n",
    "\n",
    "![image.png](http://neuralnetworksanddeeplearning.com/images/tikz13.png)\n",
    "\n",
    "\\begin{eqnarray} \n",
    "  \\frac{1}{1+\\exp(-\\sum_j w_j x_j-b)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "|   |x_0001|x_0010|x_0100|x_1000|\n",
    "|---|---|---|---|---|\n",
    "|b  |0  |0  |0  |0  |\n",
    "|w_0|-1000|-1000|-1000|-1000|\n",
    "|w_1|**1000**|-1000|-1000|-1000|\n",
    "|w_2|-1000|**1000**|-1000|-1000|\n",
    "|w_3|**1000**|**1000**|-1000|-1000|\n",
    "|w_4|-1000|-1000|**1000**|-1000|\n",
    "|w_5|**1000**|-1000|**1000**|-1000|\n",
    "|w_6|-1000|-1000|**1000**|-1000|\n",
    "|w_7|**1000**|-1000|**1000**|-1000|\n",
    "|w_8|-1000|-1000|-1000|**1000**|\n",
    "|w_9|**1000**|-1000|-1000|**1000**|\n",
    "\n",
    "\n",
    "# [Learning with gradient descent](http://neuralnetworksanddeeplearning.com/chap1.html#learning_with_gradient_descent)\n",
    "We'll use the MNIST data set, which contains tens of thousands of scanned images of handwritten digits, together with their correct classifications. http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "We'll use the notation x to denote a training input. It'll be convenient to regard each training input x as a 28×28=784-dimensional vector. Each entry in the vector represents the grey value for a single pixel in the image. We'll denote the corresponding desired output by y=y(x), where y is a 10-dimensional vector. For example, if a particular training image, x, depicts a 6, then y(x)=(0,0,0,0,0,0,1,0,0,0)T is the desired output from the network.\n",
    "\n",
    "What we'd like is an algorithm which lets us find weights and biases so that the output from the network approximates y(x) for all training inputs x. To quantify how well we're achieving this goal we define a **cost function** (Sometimes referred to as a **loss** or **objective function**):\n",
    "\n",
    "\\begin{eqnarray}  C(w,b) \\equiv\n",
    "  \\frac{1}{2n} \\sum_x \\| y(x) - a\\|^2\n",
    "\\end{eqnarray}\n",
    "\n",
    "Here, w denotes the collection of all weights in the network, b all the biases, n is the total number of training inputs, a is the vector of outputs from the network when x is input, and the sum is over all training inputs, x.\n",
    "\n",
    "The aim of our training algorithm will be to minimize the cost C(w,b) as a function of the weights and biases. In other words, we want to find a set of weights and biases which make the cost as small as possible. We'll do that using an algorithm known as gradient descent.\n",
    "\n",
    "\\begin{eqnarray} \n",
    "  \\Delta C \\approx \\frac{\\partial C}{\\partial v_1} \\Delta v_1 +\n",
    "  \\frac{\\partial C}{\\partial v_2} \\Delta v_2\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray} \n",
    "  \\nabla C \\equiv \\left( \\frac{\\partial C}{\\partial v_1}, \n",
    "  \\frac{\\partial C}{\\partial v_2} \\right)^T\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray} \n",
    "  \\Delta C \\approx \\nabla C \\cdot \\Delta v\n",
    "\\end{eqnarray}\n",
    "\n",
    "Suppose we choose:\n",
    "\\begin{eqnarray}\n",
    "  \\Delta v = -\\eta \\nabla C,\n",
    "\\end{eqnarray}\n",
    "\n",
    "where η is a small, positive parameter (known as the learning rate).\n",
    "\\begin{eqnarray}\n",
    "  \\Delta C \\approx -\\eta\n",
    "  \\nabla C \\cdot \\nabla C = -\\eta \\|\\nabla C\\|^2\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\| \\nabla C\n",
    "\\|^2 \\geq 0\n",
    "\\Delta C \\leq 0\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
